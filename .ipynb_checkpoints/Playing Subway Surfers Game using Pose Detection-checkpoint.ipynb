{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **<center><font style=\"color:rgb(100,109,254)\">Playing Subway Surfers Game using Pose Detection</font> </center>**\n",
    "\n",
    "\n",
    "<img src='https://drive.google.com/uc?export=download&id=1Msiu4noiq5NKViqXX8TE-6sei6ycS1Xx'>\n",
    "\n",
    "<img src='https://drive.google.com/uc?export=download&id=1bREfnsfCWjVyMRjXM0kI0V33kRQ7f_dY'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **<font style=\"color:rgb(134,19,348)\"> Import the Libraries</font>**\n",
    "\n",
    "We will start by importing the required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\91768\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import pyautogui\n",
    "from time import time\n",
    "from math import hypot\n",
    "import mediapipe as mp\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **<font style=\"color:rgb(134,19,348)\">Initialize the Pose Detection Model</font>**\n",
    "\n",
    "After that we will need to initialize the **`mp.solutions.pose`** class and then call the **`mp.solutions.pose.Pose()`** function  with appropriate arguments and also initialize **`mp.solutions.drawing_utils`** class that is needed to visualize the landmarks after detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize mediapipe pose class.\n",
    "mp_pose = mp.solutions.pose\n",
    "\n",
    "# Setup the Pose function for images.\n",
    "pose_image = mp_pose.Pose(static_image_mode=True, min_detection_confidence=0.5, model_complexity=1)\n",
    "\n",
    "# Setup the Pose function for videos.\n",
    "pose_video = mp_pose.Pose(static_image_mode=False, model_complexity=1, min_detection_confidence=0.7,\n",
    "                          min_tracking_confidence=0.7)\n",
    "\n",
    "# Initialize mediapipe drawing class.\n",
    "mp_drawing = mp.solutions.drawing_utils "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **<font style=\"color:rgb(134,19,348)\">Step 1: Perform Pose Detection</font>**\n",
    "\n",
    "To implement the game control mechanisms, we will need the current pose info of the person controlling the game, as our intention is to control the character with the movement of the person in the frame. We want the game's character to move left, right, jump and crouch with the identical movements of the person.\n",
    "\n",
    "So we will create a function **`detectPose()`** that will take an image as input and perform pose detection on the person in the image using the mediapipe's pose detection solution to get **thirty-three 3D landmarks** on the body and the function will display the results or return them depending upon the passed arguments.\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?export=download&id=1CDO0KiXZEOuWc7xLEm7EFLLQf2hydCoI\">\n",
    "\n",
    "This function is quite similar to the one we had created in the previous post. The only difference is that we are not plotting the pose landmarks in 3D and we are passing a few more optional arguments to the function **`mp.solutions.drawing_utils.draw_landmarks()`** to specify the drawing style.\n",
    "\n",
    "You probably do not want to lose control of the game's character whenever some other person comes into the frame (and starts controlling the character), so that annoying scenario is already taken care of, as the solution we are using only detects the landmarks of the most prominent person in the image.\n",
    "\n",
    "So you do not need to worry about losing control as long as you are the most prominent person in the frame as it will automatically ignore the people in the background."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detectPose(image, pose, draw=False, display=False):\n",
    "    '''\n",
    "    This function performs the pose detection on the most prominent person in an image.\n",
    "    Args:\n",
    "        image:   The input image with a prominent person whose pose landmarks needs to be detected.\n",
    "        pose:    The pose function required to perform the pose detection.\n",
    "        draw:    A boolean value that is if set to true the function draw pose landmarks on the output image. \n",
    "        display: A boolean value that is if set to true the function displays the original input image, and the \n",
    "                 resultant image and returns nothing.\n",
    "    Returns:\n",
    "        output_image: The input image with the detected pose landmarks drawn if it was specified.\n",
    "        results:      The output of the pose landmarks detection on the input image.\n",
    "    '''\n",
    "    \n",
    "    # Create a copy of the input image.\n",
    "    output_image = image.copy()\n",
    "    \n",
    "    # Convert the image from BGR into RGB format.\n",
    "    imageRGB = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Perform the Pose Detection.\n",
    "    results = pose.process(imageRGB)\n",
    "    \n",
    "    # Check if any landmarks are detected and are specified to be drawn.\n",
    "    if results.pose_landmarks and draw:\n",
    "    \n",
    "        # Draw Pose Landmarks on the output image.\n",
    "        mp_drawing.draw_landmarks(image=output_image, landmark_list=results.pose_landmarks,\n",
    "                                  connections=mp_pose.POSE_CONNECTIONS,\n",
    "                                  landmark_drawing_spec=mp_drawing.DrawingSpec(color=(255,255,255),\n",
    "                                                                               thickness=3, circle_radius=3),\n",
    "                                  connection_drawing_spec=mp_drawing.DrawingSpec(color=(49,125,237),\n",
    "                                                                               thickness=2, circle_radius=2))\n",
    "\n",
    "    # Check if the original input image and the resultant image are specified to be displayed.\n",
    "    if display:\n",
    "    \n",
    "        # Display the original input image and the resultant image.\n",
    "        plt.figure(figsize=[22,22])\n",
    "        plt.subplot(121);plt.i\n",
    "        mshow(image[:,:,::-1]);plt.title(\"Original Image\");plt.axis('off');\n",
    "        plt.subplot(122);plt.imshow(output_image[:,:,::-1]);plt.title(\"Output Image\");plt.axis('off');\n",
    "        \n",
    "    # Otherwise\n",
    "    else:\n",
    "\n",
    "        # Return the output image and the results of pose landmarks detection.\n",
    "        return output_image, results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will test the function **`detectPose()`** created above to perform pose detection on a sample image and display the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It worked pretty well! if you want you can test the function on other images too by just changing the value of the variable **`IMG_PATH`** in the cell above, it will work fine as long as there is a prominent person in the image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **<font style=\"color:rgb(134,19,348)\">Step 2: Control Starting Mechanism</font>**\n",
    "\n",
    "In this step, we will implement the game starting mechanism, what we want is to start the game whenever the most prominent person in the image/frame joins his both hands together. So we will create a function **`checkHandsJoined()`** that will check whether the hands of the person in an image are joined or not. \n",
    "\n",
    "The function **`checkHandsJoined()`** will take in the results of the pose detection returned by the function **`detectPose()`** and will use the **`LEFT_WRIST`** and **`RIGHT_WRIST`** landmarks coordinates from the list of thirty-three landmarks, to calculate the euclidean distance between the hands of the person.\n",
    "\n",
    "<img src='https://drive.google.com/uc?export=download&id=1p76mydN2UXU_0lMpQD5pzyM01ec2PQDy' width=300>\n",
    "\n",
    "And then utilize an appropriate threshold value to compare with and check whether the hands of the person in the image/frame are joined or not and will display or return the results depending upon the passed arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkHandsJoined(image, results, draw=False, display=False):\n",
    "    '''\n",
    "    This function checks whether the hands of the person are joined or not in an image.\n",
    "    Args:\n",
    "        image:   The input image with a prominent person whose hands status (joined or not) needs to be classified.\n",
    "        results: The output of the pose landmarks detection on the input image.\n",
    "        draw:    A boolean value that is if set to true the function writes the hands status & distance on the output image. \n",
    "        display: A boolean value that is if set to true the function displays the resultant image and returns nothing.\n",
    "    Returns:\n",
    "        output_image: The same input image but with the classified hands status written, if it was specified.\n",
    "        hand_status:  The classified status of the hands whether they are joined or not.\n",
    "    '''\n",
    "    \n",
    "    # Get the height and width of the input image.\n",
    "    height, width, _ = image.shape\n",
    "    \n",
    "    # Create a copy of the input image to write the hands status label on.\n",
    "    output_image = image.copy()\n",
    "    \n",
    "    # Get the left wrist landmark x and y coordinates.\n",
    "    left_wrist_landmark = (results.pose_landmarks.landmark[mp_pose.PoseLandmark.LEFT_WRIST].x * width,\n",
    "                          results.pose_landmarks.landmark[mp_pose.PoseLandmark.LEFT_WRIST].y * height)\n",
    "\n",
    "    # Get the right wrist landmark x and y coordinates.\n",
    "    right_wrist_landmark = (results.pose_landmarks.landmark[mp_pose.PoseLandmark.RIGHT_WRIST].x * width,\n",
    "                           results.pose_landmarks.landmark[mp_pose.PoseLandmark.RIGHT_WRIST].y * height)\n",
    "    \n",
    "    # Calculate the euclidean distance between the left and right wrist.\n",
    "    euclidean_distance = int(hypot(left_wrist_landmark[0] - right_wrist_landmark[0],\n",
    "                                   left_wrist_landmark[1] - right_wrist_landmark[1]))\n",
    "    \n",
    "    # Compare the distance between the wrists with a appropriate threshold to check if both hands are joined.\n",
    "    if euclidean_distance < 130:\n",
    "        \n",
    "        # Set the hands status to joined.\n",
    "        hand_status = 'Hands Joined'\n",
    "        \n",
    "        # Set the color value to green.\n",
    "        color = (0, 255, 0)\n",
    "        \n",
    "    # Otherwise.    \n",
    "    else:\n",
    "        \n",
    "        # Set the hands status to not joined.\n",
    "        hand_status = 'Hands Not Joined'\n",
    "        \n",
    "        # Set the color value to red.\n",
    "        color = (0, 0, 255)\n",
    "        \n",
    "    # Check if the Hands Joined status and hands distance are specified to be written on the output image.\n",
    "    if draw:\n",
    "\n",
    "        # Write the classified hands status on the image. \n",
    "        cv2.putText(output_image, hand_status, (10, 30), cv2.FONT_HERSHEY_PLAIN, 2, color, 3)\n",
    "        \n",
    "        # Write the the distance between the wrists on the image. \n",
    "        cv2.putText(output_image, f'Distance: {euclidean_distance}', (10, 70),\n",
    "                    cv2.FONT_HERSHEY_PLAIN, 2, color, 3)\n",
    "        \n",
    "    # Check if the output image is specified to be displayed.\n",
    "    if display:\n",
    "\n",
    "        # Display the output image.\n",
    "        plt.figure(figsize=[10,10])\n",
    "        plt.imshow(output_image[:,:,::-1]);plt.title(\"Output Image\");plt.axis('off');\n",
    "    \n",
    "    # Otherwise\n",
    "    else:\n",
    "    \n",
    "        # Return the output image and the classified hands status indicating whether the hands are joined or not.\n",
    "        return output_image, hand_status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will test the function **`checkHandsJoined()`** created above on a real-time webcam feed to check whether it is working as we had expected or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\91768\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\google\\protobuf\\symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
      "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 26\u001b[0m\n\u001b[0;32m     23\u001b[0m frame_height, frame_width, _ \u001b[38;5;241m=\u001b[39m frame\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Perform the pose detection on the frame.\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m frame, results \u001b[38;5;241m=\u001b[39m \u001b[43mdetectPose\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpose_video\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdraw\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# Check if the pose landmarks in the frame are detected.\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m results\u001b[38;5;241m.\u001b[39mpose_landmarks:\n\u001b[0;32m     30\u001b[0m         \n\u001b[0;32m     31\u001b[0m     \u001b[38;5;66;03m# Check if the left and right hands are joined.\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[3], line 22\u001b[0m, in \u001b[0;36mdetectPose\u001b[1;34m(image, pose, draw, display)\u001b[0m\n\u001b[0;32m     19\u001b[0m imageRGB \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(image, cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2RGB)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Perform the Pose Detection.\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mpose\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimageRGB\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Check if any landmarks are detected and are specified to be drawn.\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m results\u001b[38;5;241m.\u001b[39mpose_landmarks \u001b[38;5;129;01mand\u001b[39;00m draw:\n\u001b[0;32m     26\u001b[0m \n\u001b[0;32m     27\u001b[0m     \u001b[38;5;66;03m# Draw Pose Landmarks on the output image.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\91768\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\mediapipe\\python\\solutions\\pose.py:185\u001b[0m, in \u001b[0;36mPose.process\u001b[1;34m(self, image)\u001b[0m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess\u001b[39m(\u001b[38;5;28mself\u001b[39m, image: np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NamedTuple:\n\u001b[0;32m    165\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Processes an RGB image and returns the pose landmarks on the most prominent person detected.\u001b[39;00m\n\u001b[0;32m    166\u001b[0m \n\u001b[0;32m    167\u001b[0m \u001b[38;5;124;03m  Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    182\u001b[0m \u001b[38;5;124;03m         \"enable_segmentation\" is set to true.\u001b[39;00m\n\u001b[0;32m    183\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 185\u001b[0m   results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimage\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    186\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m results\u001b[38;5;241m.\u001b[39mpose_landmarks:  \u001b[38;5;66;03m# pytype: disable=attribute-error\u001b[39;00m\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m landmark \u001b[38;5;129;01min\u001b[39;00m results\u001b[38;5;241m.\u001b[39mpose_landmarks\u001b[38;5;241m.\u001b[39mlandmark:  \u001b[38;5;66;03m# pytype: disable=attribute-error\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\91768\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\mediapipe\\python\\solution_base.py:365\u001b[0m, in \u001b[0;36mSolutionBase.process\u001b[1;34m(self, input_data)\u001b[0m\n\u001b[0;32m    359\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    360\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_graph\u001b[38;5;241m.\u001b[39madd_packet_to_input_stream(\n\u001b[0;32m    361\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream_name,\n\u001b[0;32m    362\u001b[0m         packet\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_packet(input_stream_type,\n\u001b[0;32m    363\u001b[0m                                  data)\u001b[38;5;241m.\u001b[39mat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_simulated_timestamp))\n\u001b[1;32m--> 365\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_graph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait_until_idle\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    366\u001b[0m \u001b[38;5;66;03m# Create a NamedTuple object where the field names are mapping to the graph\u001b[39;00m\n\u001b[0;32m    367\u001b[0m \u001b[38;5;66;03m# output stream names.\u001b[39;00m\n\u001b[0;32m    368\u001b[0m solution_outputs \u001b[38;5;241m=\u001b[39m collections\u001b[38;5;241m.\u001b[39mnamedtuple(\n\u001b[0;32m    369\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSolutionOutputs\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_stream_type_info\u001b[38;5;241m.\u001b[39mkeys())\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Initialize the VideoCapture object to read from the webcam.\n",
    "camera_video = cv2.VideoCapture(0)\n",
    "camera_video.set(3,1280)\n",
    "camera_video.set(4,960)\n",
    "\n",
    "# Create named window for resizing purposes.\n",
    "cv2.namedWindow('Hands Joined?', cv2.WINDOW_NORMAL)\n",
    "\n",
    "# Iterate until the webcam is accessed successfully.\n",
    "while camera_video.isOpened():\n",
    "    \n",
    "    # Read a frame.\n",
    "    ok, frame = camera_video.read()\n",
    "    \n",
    "    # Check if frame is not read properly then continue to the next iteration to read the next frame.\n",
    "    if not ok:\n",
    "        continue\n",
    "    \n",
    "    # Flip the frame horizontally for natural (selfie-view) visualization.\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    \n",
    "    # Get the height and width of the frame of the webcam video.\n",
    "    frame_height, frame_width, _ = frame.shape\n",
    "    \n",
    "    # Perform the pose detection on the frame.\n",
    "    frame, results = detectPose(frame, pose_video, draw=True)\n",
    "    \n",
    "    # Check if the pose landmarks in the frame are detected.\n",
    "    if results.pose_landmarks:\n",
    "            \n",
    "        # Check if the left and right hands are joined.\n",
    "        frame, _ = checkHandsJoined(frame, results, draw=True)\n",
    "                \n",
    "    # Display the frame.\n",
    "    cv2.imshow('Hands Joined?', frame)\n",
    "    \n",
    "    # Wait for 1ms. If a key is pressed, retreive the ASCII code of the key.\n",
    "    k = cv2.waitKey(1) & 0xFF\n",
    "    \n",
    "    # Check if 'ESC' is pressed and break the loop.\n",
    "    if(k == 27):\n",
    "        break\n",
    "\n",
    "# Release the VideoCapture Object and close the windows.\n",
    "camera_video.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Woah! I am stunned, the pose detection solution is best known for its speed which is reflecting in the results as the **`distance`** and the **`hands status`** are updating very fast and are also highly accurate.\n",
    "\n",
    "## **<font style=\"color:rgb(134,19,348)\">Step 3: Control Horizontal Movements</font>**\n",
    "\n",
    "Now comes the implementation of the left and right movements control mechanism of the game's character, what we want to do is to make the game's character move left and right with the horizontal movements of the person in the image/frame.\n",
    "\n",
    "So we will create a function **`checkLeftRight()`** that will take in the pose detection results returned by the function **`detectPose()`** and will use the x-coordinates of the **`RIGHT_SHOULDER`** and **`LEFT_SHOULDER`** landmarks to determine the horizontal position (**`Left`, `Right` or `Center`**) in the frame after comparing the landmarks with the x-coordinate of the center of the image. \n",
    "\n",
    "The function will visualize or return the resultant image and the horizontal position of the person depending upon the passed arguments.\n",
    "\n",
    "<img src='https://drive.google.com/uc?export=download&id=1LhngpRrIJYMYIlKMnUep4YxdXMni3RvI'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkLeftRight(image, results, draw=False, display=False):\n",
    "    '''\n",
    "    This function finds the horizontal position (left, center, right) of the person in an image.\n",
    "    Args:\n",
    "        image:   The input image with a prominent person whose the horizontal position needs to be found.\n",
    "        results: The output of the pose landmarks detection on the input image.\n",
    "        draw:    A boolean value that is if set to true the function writes the horizontal position on the output image. \n",
    "        display: A boolean value that is if set to true the function displays the resultant image and returns nothing.\n",
    "    Returns:\n",
    "        output_image:         The same input image but with the horizontal position written, if it was specified.\n",
    "        horizontal_position:  The horizontal position (left, center, right) of the person in the input image.\n",
    "    '''\n",
    "    \n",
    "    # Declare a variable to store the horizontal position (left, center, right) of the person.\n",
    "    horizontal_position = None\n",
    "    \n",
    "    # Get the height and width of the image.\n",
    "    height, width, _ = image.shape\n",
    "    \n",
    "    # Create a copy of the input image to write the horizontal position on.\n",
    "    output_image = image.copy()\n",
    "    \n",
    "    # Retreive the x-coordinate of the left shoulder landmark.\n",
    "    left_x = int(results.pose_landmarks.landmark[mp_pose.PoseLandmark.RIGHT_SHOULDER].x * width)\n",
    "\n",
    "    # Retreive the x-corrdinate of the right shoulder landmark.\n",
    "    right_x = int(results.pose_landmarks.landmark[mp_pose.PoseLandmark.LEFT_SHOULDER].x * width)\n",
    "    \n",
    "    # Check if the person is at left that is when both shoulder landmarks x-corrdinates\n",
    "    # are less than or equal to the x-corrdinate of the center of the image.\n",
    "    if (right_x <= width//2 and left_x <= width//2):\n",
    "        \n",
    "        # Set the person's position to left.\n",
    "        horizontal_position = 'Left'\n",
    "\n",
    "    # Check if the person is at right that is when both shoulder landmarks x-corrdinates\n",
    "    # are greater than or equal to the x-corrdinate of the center of the image.\n",
    "    elif (right_x >= width//2 and left_x >= width//2):\n",
    "        \n",
    "        # Set the person's position to right.\n",
    "        horizontal_position = 'Right'\n",
    "    \n",
    "    # Check if the person is at center that is when right shoulder landmark x-corrdinate is greater than or equal to\n",
    "    # and left shoulder landmark x-corrdinate is less than or equal to the x-corrdinate of the center of the image.\n",
    "    elif (right_x >= width//2 and left_x <= width//2):\n",
    "        \n",
    "        # Set the person's position to center.\n",
    "        horizontal_position = 'Center'\n",
    "        \n",
    "    # Check if the person's horizontal position and a line at the center of the image is specified to be drawn.\n",
    "    if draw:\n",
    "\n",
    "        # Write the horizontal position of the person on the image. \n",
    "        cv2.putText(output_image, horizontal_position, (5, height - 10), cv2.FONT_HERSHEY_PLAIN, 2, (255, 255, 255), 3)\n",
    "        \n",
    "        # Draw a line at the center of the image.\n",
    "        cv2.line(output_image, (width//2, 0), (width//2, height), (255, 255, 255), 2)\n",
    "        \n",
    "    # Check if the output image is specified to be displayed.\n",
    "    if display:\n",
    "\n",
    "        # Display the output image.\n",
    "        plt.figure(figsize=[10,10])\n",
    "        plt.imshow(output_image[:,:,::-1]);plt.title(\"Output Image\");plt.axis('off');\n",
    "    \n",
    "    # Otherwise\n",
    "    else:\n",
    "    \n",
    "        # Return the output image and the person's horizontal position.\n",
    "        return output_image, horizontal_position"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will test the function **`checkLeftRight()`** created above on a real-time webcam feed and will visualize the results updating in real-time with the horizontal movements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the VideoCapture object to read from the webcam.\n",
    "camera_video = cv2.VideoCapture(0)\n",
    "camera_video.set(3,1280)\n",
    "camera_video.set(4,960)\n",
    "\n",
    "# Create named window for resizing purposes.\n",
    "cv2.namedWindow('Horizontal Movements', cv2.WINDOW_NORMAL)\n",
    "\n",
    "# Iterate until the webcam is accessed successfully.\n",
    "while camera_video.isOpened():\n",
    "    \n",
    "    # Read a frame.\n",
    "    ok, frame = camera_video.read()\n",
    "    \n",
    "    # Check if frame is not read properly then continue to the next iteration to read the next frame.\n",
    "    if not ok:\n",
    "        continue\n",
    "    \n",
    "    # Flip the frame horizontally for natural (selfie-view) visualization.\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    \n",
    "    # Get the height and width of the frame of the webcam video.\n",
    "    frame_height, frame_width, _ = frame.shape\n",
    "    \n",
    "    # Perform the pose detection on the frame.\n",
    "    frame, results = detectPose(frame, pose_video, draw=True)\n",
    "    \n",
    "    # Check if the pose landmarks in the frame are detected.\n",
    "    if results.pose_landmarks:\n",
    "            \n",
    "        # Check the horizontal position of the person in the frame.\n",
    "        frame, _ = checkLeftRight(frame, results, draw=True)\n",
    "                \n",
    "    # Display the frame.\n",
    "    cv2.imshow('Horizontal Movements', frame)\n",
    "    \n",
    "    # Wait for 1ms. If a a key is pressed, retreive the ASCII code of the key.\n",
    "    k = cv2.waitKey(1) & 0xFF\n",
    "    \n",
    "    # Check if 'ESC' is pressed and break the loop.\n",
    "    if(k == 27):\n",
    "        break\n",
    "\n",
    "# Release the VideoCapture Object and close the windows.\n",
    "camera_video.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool! the speed and accuracy of this model never fail to impress me.\n",
    "\n",
    "## **<font style=\"color:rgb(134,19,348)\">Step 4: Control Vertical Movements</font>**\n",
    "\n",
    "In this one, we will implement the jump and crouch control mechanism of the game's character, what we want is to make the game's character jump and crouch whenever the person in the image/frame jumps and crouches. \n",
    "\n",
    "\n",
    "So we will create a function **`checkJumpCrouch()`** that will check whether the posture of the person in an image is `Jumping`, `Crouching` or `Standing` by utilizing the results of pose detection by the function **`detectPose()`**.\n",
    "\n",
    "The function **`checkJumpCrouch()`** will retrieve the **`RIGHT_SHOULDER`** and **`LEFT_SHOULDER`** landmarks from the list to calculate the y-coordinate of the midpoint of both shoulders and will determine the posture of the person by doing a comparison with an appropriate threshold value. \n",
    "\n",
    "The threshold (**`MID_Y`**) will be the approximate y-coordinate of the midpoint of both shoulders of the person while in standing posture. It will be calculated before starting the game in the **`Step 6:` Build the Final Application** and will be passed to the function **`checkJumpCrouch()`**. \n",
    "\n",
    "But the issue with this approach is that the midpoint of both shoulders of the person while in standing posture will not always be exactly same as it will vary when the person will move closer or further to the camera. \n",
    "\n",
    "To tackle this issue we will add and subtract a margin to the threshold to get an upper and lower bound as shown in the image below.\n",
    "\n",
    "<img src='https://drive.google.com/uc?export=download&id=1fNAsoK964C4ASIkX6UXJNtvooZjlZNQT'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkJumpCrouch(image, results, MID_Y=250, draw=False, display=False):\n",
    "    '''\n",
    "    This function checks the posture (Jumping, Crouching or Standing) of the person in an image.\n",
    "    Args:\n",
    "        image:   The input image with a prominent person whose the posture needs to be checked.\n",
    "        results: The output of the pose landmarks detection on the input image.\n",
    "        MID_Y:   The intial center y-coordinate of both shoulders landmarks of the person recorded during starting\n",
    "                 the game. This will give the idea of the person's height when he is standing straight.\n",
    "        draw:    A boolean value that is if set to true the function writes the posture on the output image. \n",
    "        display: A boolean value that is if set to true the function displays the resultant image and returns nothing.\n",
    "    Returns:\n",
    "        output_image: The input image with the person's posture written, if it was specified.\n",
    "        posture:      The posture (Jumping, Crouching or Standing) of the person in an image.\n",
    "    '''\n",
    "    \n",
    "    # Get the height and width of the image.\n",
    "    height, width, _ = image.shape\n",
    "    \n",
    "    # Create a copy of the input image to write the posture label on.\n",
    "    output_image = image.copy()\n",
    "    \n",
    "    # Retreive the y-coordinate of the left shoulder landmark.\n",
    "    left_y = int(results.pose_landmarks.landmark[mp_pose.PoseLandmark.RIGHT_SHOULDER].y * height)\n",
    "\n",
    "    # Retreive the y-coordinate of the right shoulder landmark.\n",
    "    right_y = int(results.pose_landmarks.landmark[mp_pose.PoseLandmark.LEFT_SHOULDER].y * height)\n",
    "\n",
    "    # Calculate the y-coordinate of the mid-point of both shoulders.\n",
    "    actual_mid_y = abs(right_y + left_y) // 2\n",
    "    \n",
    "    # Calculate the upper and lower bounds of the threshold.\n",
    "    lower_bound = MID_Y-15\n",
    "    upper_bound = MID_Y+100\n",
    "    \n",
    "    # Check if the person has jumped that is when the y-coordinate of the mid-point \n",
    "    # of both shoulders is less than the lower bound.\n",
    "    if (actual_mid_y < lower_bound):\n",
    "        \n",
    "        # Set the posture to jumping.\n",
    "        posture = 'Jumping'\n",
    "    \n",
    "    # Check if the person has crouched that is when the y-coordinate of the mid-point \n",
    "    # of both shoulders is greater than the upper bound.\n",
    "    elif (actual_mid_y > upper_bound):\n",
    "        \n",
    "        # Set the posture to crouching.\n",
    "        posture = 'Crouching'\n",
    "    \n",
    "    # Otherwise the person is standing and the y-coordinate of the mid-point \n",
    "    # of both shoulders is between the upper and lower bounds.    \n",
    "    else:\n",
    "        \n",
    "        # Set the posture to Standing straight.\n",
    "        posture = 'Standing'\n",
    "        \n",
    "    # Check if the posture and a horizontal line at the threshold is specified to be drawn.\n",
    "    if draw:\n",
    "\n",
    "        # Write the posture of the person on the image. \n",
    "        cv2.putText(output_image, posture, (5, height - 50), cv2.FONT_HERSHEY_PLAIN, 2, (255, 255, 255), 3)\n",
    "        \n",
    "        # Draw a line at the intial center y-coordinate of the person (threshold).\n",
    "        cv2.line(output_image, (0, MID_Y),(width, MID_Y),(255, 255, 255), 2)\n",
    "        \n",
    "    # Check if the output image is specified to be displayed.\n",
    "    if display:\n",
    "\n",
    "        # Display the output image.\n",
    "        plt.figure(figsize=[10,10])\n",
    "        plt.imshow(output_image[:,:,::-1]);plt.title(\"Output Image\");plt.axis('off');\n",
    "    \n",
    "    # Otherwise\n",
    "    else:\n",
    "    \n",
    "        # Return the output image and posture indicating whether the person is standing straight or has jumped, or crouched.\n",
    "        return output_image, posture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will test the function **`checkJumpCrouch()`** created above on the real-time webcam feed and will visualize the resultant frames. For testing purposes, we will be using a default value of the threshold, that if you want you can tune manually set according to your height."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the VideoCapture object to read from the webcam.\n",
    "camera_video = cv2.VideoCapture(0)\n",
    "camera_video.set(3,1280)\n",
    "camera_video.set(4,960)\n",
    "\n",
    "# Create named window for resizing purposes.\n",
    "cv2.namedWindow('Verticial Movements', cv2.WINDOW_NORMAL)\n",
    "\n",
    "# Iterate until the webcam is accessed successfully.\n",
    "while camera_video.isOpened():\n",
    "    \n",
    "    # Read a frame.\n",
    "    ok, frame = camera_video.read()\n",
    "    \n",
    "    # Check if frame is not read properly then continue to the next iteration to read the next frame.\n",
    "    if not ok:\n",
    "        continue\n",
    "    \n",
    "    # Flip the frame horizontally for natural (selfie-view) visualization.\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    \n",
    "    # Get the height and width of the frame of the webcam video.\n",
    "    frame_height, frame_width, _ = frame.shape\n",
    "    \n",
    "    # Perform the pose detection on the frame.\n",
    "    frame, results = detectPose(frame, pose_video, draw=True)\n",
    "    \n",
    "    # Check if the pose landmarks in the frame are detected.\n",
    "    if results.pose_landmarks:\n",
    "            \n",
    "        # Check the posture (jumping, crouching or standing) of the person in the frame. \n",
    "        frame, _ = checkJumpCrouch(frame, results, draw=True)\n",
    "                \n",
    "    # Display the frame.\n",
    "    cv2.imshow('Verticial Movements', frame)\n",
    "    \n",
    "    # Wait for 1ms. If a a key is pressed, retreive the ASCII code of the key.\n",
    "    k = cv2.waitKey(1) & 0xFF\n",
    "    \n",
    "    # Check if 'ESC' is pressed and break the loop.\n",
    "    if(k == 27):\n",
    "        break\n",
    "\n",
    "# Release the VideoCapture Object and close the windows.\n",
    "camera_video.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! when I lower my shoulders at a certain range from the horizontal line (threshold), the results are **`Crouching`**, and the results are **`Standing`**, whenever my shoulders are near the horizontal line (i.e., between the upper and lower bounds), and when my shoulders are at a certain range above the horizontal line, the results are **`Jumping`**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **<font style=\"color:rgb(134,19,348)\">Step 5: Control Keyboard and Mouse with PyautoGUI</font>**\n",
    "\n",
    "The Subway Surfers character wouldn't be able to move left, right, jump or crouch unless we provide it the required keyboard inputs. Now that we have the functions **`checkHandsJoined()`**, **`checkLeftRight()`** and **`checkJumpCrouch()`**, we need to figure out a way to trigger the required keyboard keypress events, depending upon the output of the functions created above.\n",
    "\n",
    "This is where the [**`PyAutoGUI`**](https://pyautogui.readthedocs.io/en/latest/) API shines. It allows you to easily control the mouse and keyboard event through scripts. To get an idea of PyAutoGUI's capabilities, you can check this [video](https://www.youtube.com/watch?v=lfk_T6VKhTE) in which a bot is playing the game `Sushi Go Round`.\n",
    "\n",
    "To run the cells in this step, it is not recommended to use the keyboard keys (**Shift + Enter**) as the cells with keypress events will behave differently when the events will be combined with the keys *`Shift`* and *`Enter`*. You can either use the menubar **`(Cell>>Run Cell)`** or the toolbar **`(▶️Run)`** to run the cells."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see how simple it is to trigger the **`up`** arrow keypress event using pyautogui."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Press the up key.\n",
    "pyautogui.press(keys='up')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, we can trigger the **`down`** arrow or any other keypress event by replacing the argument with that key name (the argument should be a string). You can click [here](https://pyautogui.readthedocs.io/en/latest/keyboard.html#keyboard-keys) to see the list of valid arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Press the down key.\n",
    "pyautogui.press(keys='down')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To press multiple keys, we can pass a list of strings `(key names)` to the **`pyautogui.press()`** function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Press the up (4 times) and down (1 time) key.\n",
    "pyautogui.press(keys=['up', 'up', 'up', 'up', 'down'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or to press the same key multiple times, we can pass a value `(number of times we want to press the key)` to the argument **`presses`** in the **`pyautogui.press()`** function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Press the down key 4 times.\n",
    "pyautogui.press(keys='down', presses=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function presses the key(s) down and then releases up the key(s) automatically. We can also control this key press event and key release event individually by using the functions:\n",
    "\n",
    "* **`pyautogui.keyDown(key)`**: Presses and holds down the specified `key`.\n",
    "\n",
    "* **`pyautogui.keyUp(key)`**:   Releases up the specified `key`.\n",
    "\n",
    "So with the help of these functions, keys can be pressed for a longer period. Like in the cell below we will hold down the **`shift`** key and press the **`enter`** key (two times) to run the two cells below this one and then we will release the **`shift`** key. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hold down the shift key.\n",
    "pyautogui.keyDown(key='shift') \n",
    "\n",
    "# Press the enter key two times.\n",
    "pyautogui.press(keys='enter', presses=2) \n",
    "\n",
    "# Release the shift key.\n",
    "pyautogui.keyUp(key='shift')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello!\n"
     ]
    }
   ],
   "source": [
    "# This cell will run automatically due to keypress events in the previous cell.\n",
    "print('Hello!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Happy Learning!\n"
     ]
    }
   ],
   "source": [
    "# This cell will also run automatically due to those keypress events.\n",
    "print('Happy Learning!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will hold down the **`shift`** key and press the **`tab`** key and then we will release the **`shift`** key. This will switch the tab of your browser so make sure to have multiple tabs before running the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hold down the shift key.\n",
    "pyautogui.keyDown(key='ctrl') \n",
    "\n",
    "# Press the tab key.\n",
    "pyautogui.press(keys='tab') \n",
    "\n",
    "# Release the shift key.\n",
    "pyautogui.keyUp(key='ctrl')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To trigger the mouse keypress events, we can use [**`pyautogui.click()`**](https://pyautogui.readthedocs.io/en/latest/mouse.html#mouse-clicks) function and to specify the mouse button that we want to press, we can pass the values **`left`**, **`middle`**, or **`right`** to the argument **`button`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Press the mouse right button. It will open up the menu.\n",
    "pyautogui.click(button='right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also move the mouse cursor to a specific position on the screen by specifying the x and y-coordinate values to the arguments **`x`** and **`y`** respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move to 1300, 800, then click the right mouse button\n",
    "pyautogui.click(x=1300, y=800, button='right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **<font style=\"color:rgb(134,19,348)\">Step 6: Build the Final Application</font>**\n",
    "\n",
    "In the final step, we will have to combine all the components to build the final application. \n",
    "\n",
    "We will use the outputs of the functions created above **`checkHandsJoined()`** (to start the game), **`checkLeftRight()`** (control horizontal movements) and **`checkJumpCrouch()`** (control vertical movements) to trigger the relevant keyboard and mouse events and control the game's character with our body movements.\n",
    "\n",
    "Now we will run the cell below and click [here](https://poki.com/en/g/subway-surfers/) to play the game in our browser using our body gestures and movements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cv2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Initialize the VideoCapture object to read from the webcam.\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m camera_video \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241m.\u001b[39mVideoCapture(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m      3\u001b[0m camera_video\u001b[38;5;241m.\u001b[39mset(\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m1280\u001b[39m)\n\u001b[0;32m      4\u001b[0m camera_video\u001b[38;5;241m.\u001b[39mset(\u001b[38;5;241m4\u001b[39m,\u001b[38;5;241m960\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'cv2' is not defined"
     ]
    }
   ],
   "source": [
    "# Initialize the VideoCapture object to read from the webcam.\n",
    "camera_video = cv2.VideoCapture(0)\n",
    "camera_video.set(3,1280)\n",
    "camera_video.set(4,960)\n",
    "\n",
    "# Create named window for resizing purposes.\n",
    "cv2.namedWindow('Subway Surfers with Pose Detection', cv2.WINDOW_NORMAL)\n",
    " \n",
    "# Initialize a variable to store the time of the previous frame.\n",
    "time1 = 0\n",
    "\n",
    "# Initialize a variable to store the state of the game (started or not).\n",
    "game_started = False   \n",
    "\n",
    "# Initialize a variable to store the index of the current horizontal position of the person.\n",
    "# At Start the character is at center so the index is 1 and it can move left (value 0) and right (value 2).\n",
    "x_pos_index = 1\n",
    "\n",
    "# Initialize a vari able to store the index of the current vertical posture of the person.\n",
    "# At Start the person is standing so the index is 1 and he can crouch (value 0) and jump (value 2).\n",
    "y_pos_index = 1\n",
    "\n",
    "# Declate a variable to store the intial y-coordinate of the mid-point of both shoulders of the person.\n",
    "MID_Y = None\n",
    "\n",
    "# Initialize a counter to store count of the number of consecutive frames with person's hands joined.\n",
    "counter = 0\n",
    "\n",
    "# Initialize the number of consecutive frames on which we want to check if person hands joined before starting the game.\n",
    "num_of_frames = 10\n",
    "\n",
    "# Iterate until the webcam is accessed successfully.\n",
    "while camera_video.isOpened():\n",
    "    \n",
    "    # Read a frame.\n",
    "    ok, frame = camera_video.read()\n",
    "    \n",
    "    # Check if frame is not read properly then continue to the next iteration to read the next frame.\n",
    "    if not ok:\n",
    "        continue\n",
    "    \n",
    "    # Flip the frame horizontally for natural (selfie-view) visualization.\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    \n",
    "    # Get the height and width of the frame of the webcam video.\n",
    "    frame_height, frame_width, _ = frame.shape\n",
    "    \n",
    "    # Perform the pose detection on the frame.\n",
    "    frame, results = detectPose(frame, pose_video, draw=game_started)\n",
    "    \n",
    "    # Check if the pose landmarks in the frame are detected.\n",
    "    if results.pose_landmarks:\n",
    "        \n",
    "        # Check if the game has started\n",
    "        if game_started:\n",
    "            \n",
    "            # Commands to control the horizontal movements of the character.\n",
    "            #--------------------------------------------------------------------------------------------------------------\n",
    "            \n",
    "            # Get horizontal position of the person in the frame.\n",
    "            frame, horizontal_position = checkLeftRight(frame, results, draw=True)\n",
    "            \n",
    "            # Check if the person has moved to left from center or to center from right.\n",
    "            if (horizontal_position=='Left' and x_pos_index!=0) or (horizontal_position=='Center' and x_pos_index==2):\n",
    "                \n",
    "                # Press the left arrow key.\n",
    "                pyautogui.press('left')\n",
    "                \n",
    "                # Update the horizontal position index of the character.\n",
    "                x_pos_index -= 1               \n",
    "\n",
    "            # Check if the person has moved to Right from center or to center from left.\n",
    "            elif (horizontal_position=='Right' and x_pos_index!=2) or (horizontal_position=='Center' and x_pos_index==0):\n",
    "                \n",
    "                # Press the right arrow key.\n",
    "                pyautogui.press('right')\n",
    "                \n",
    "                # Update the horizontal position index of the character.\n",
    "                x_pos_index += 1\n",
    "            \n",
    "            #--------------------------------------------------------------------------------------------------------------\n",
    "        \n",
    "        # Otherwise if the game has not started    \n",
    "        else:\n",
    "            \n",
    "            # Write the text representing the way to start the game on the frame. \n",
    "            cv2.putText(frame, 'JOIN BOTH HANDS TO START THE GAME.', (5, frame_height - 10), cv2.FONT_HERSHEY_PLAIN,\n",
    "                        2, (0, 255, 0), 3)\n",
    "        \n",
    "        # Command to Start or resume the game.\n",
    "        #------------------------------------------------------------------------------------------------------------------\n",
    "        \n",
    "        # Check if the left and right hands are joined.\n",
    "        if checkHandsJoined(frame, results)[1] == 'Hands Joined':\n",
    "\n",
    "            # Increment the count of consecutive frames with +ve condition.\n",
    "            counter += 1\n",
    "\n",
    "            # Check if the counter is equal to the required number of consecutive frames.  \n",
    "            if counter == num_of_frames:\n",
    "\n",
    "                # Command to Start the game first time.\n",
    "                #----------------------------------------------------------------------------------------------------------\n",
    "                \n",
    "                # Check if the game has not started yet.\n",
    "                if not(game_started):\n",
    "\n",
    "                    # Update the value of the variable that stores the game state.\n",
    "                    game_started = True\n",
    "\n",
    "                    # Retreive the y-coordinate of the left shoulder landmark.\n",
    "                    left_y = int(results.pose_landmarks.landmark[mp_pose.PoseLandmark.RIGHT_SHOULDER].y * frame_height)\n",
    "\n",
    "                    # Retreive the y-coordinate of the right shoulder landmark.\n",
    "                    right_y = int(results.pose_landmarks.landmark[mp_pose.PoseLandmark.LEFT_SHOULDER].y * frame_height)\n",
    "\n",
    "                    # Calculate the intial y-coordinate of the mid-point of both shoulders of the person.\n",
    "                    MID_Y = abs(right_y + left_y) // 2\n",
    "\n",
    "                    # Move to 1300, 800, then click the left mouse button to start the game.\n",
    "                    pyautogui.click(x=1300, y=800, button='left')\n",
    "                \n",
    "                #----------------------------------------------------------------------------------------------------------\n",
    "\n",
    "                # Command to resume the game after death of the character.\n",
    "                #----------------------------------------------------------------------------------------------------------\n",
    "                \n",
    "                # Otherwise if the game has started.\n",
    "                else:\n",
    "\n",
    "                    # Press the space key.\n",
    "                    pyautogui.press('space')\n",
    "                \n",
    "                #----------------------------------------------------------------------------------------------------------\n",
    "                \n",
    "                # Update the counter value to zero.\n",
    "                counter = 0\n",
    "\n",
    "        # Otherwise if the left and right hands are not joined.        \n",
    "        else:\n",
    "\n",
    "            # Update the counter value to zero.\n",
    "            counter = 0\n",
    "            \n",
    "        #------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "        # Commands to control the vertical movements of the character.\n",
    "        #------------------------------------------------------------------------------------------------------------------\n",
    "        \n",
    "        # Check if the intial y-coordinate of the mid-point of both shoulders of the person has a value.\n",
    "        if MID_Y:\n",
    "            \n",
    "            # Get posture (jumping, crouching or standing) of the person in the frame. \n",
    "            frame, posture = checkJumpCrouch(frame, results, MID_Y, draw=True)\n",
    "            \n",
    "            # Check if the person has jumped.\n",
    "            if posture == 'Jumping' and y_pos_index == 1:\n",
    "\n",
    "                # Press the up arrow key\n",
    "                pyautogui.press('up')\n",
    "                \n",
    "                # Update the veritcal position index of  the character.\n",
    "                y_pos_index += 1 \n",
    "\n",
    "            # Check if the person has crouched.\n",
    "            elif posture == 'Crouching' and y_pos_index == 1:\n",
    "\n",
    "                # Press the down arrow key\n",
    "                pyautogui.press('down')\n",
    "                \n",
    "                # Update the veritcal position index of the character.\n",
    "                y_pos_index -= 1\n",
    "            \n",
    "            # Check if the person has stood.\n",
    "            elif posture == 'Standing' and y_pos_index   != 1:\n",
    "                \n",
    "                # Update the veritcal position index of the character.\n",
    "                y_pos_index = 1\n",
    "        \n",
    "        #------------------------------------------------------------------------------------------------------------------\n",
    "    \n",
    "    \n",
    "    # Otherwise if the pose landmarks in the frame are not detected.       \n",
    "    else:\n",
    "\n",
    "        # Update the counter value to zero.\n",
    "        counter = 0\n",
    "        \n",
    "    # Calculate the frames updates in one second\n",
    "    #----------------------------------------------------------------------------------------------------------------------\n",
    "    \n",
    "    # Set the time for this frame to the current time.\n",
    "    time2 = time()\n",
    "    \n",
    "    # Check if the difference between the previous and this frame time > 0 to avoid division by zero.\n",
    "    if (time2 - time1) > 0:\n",
    "    \n",
    "        # Calculate the number of frames per second.\n",
    "        frames_per_second = 1.0 / (time2 - time1)\n",
    "        \n",
    "        # Write the calculated number of frames per second on the frame. \n",
    "        cv2.putText(frame, 'FPS: {}'.format(int(frames_per_second)), (10, 30),cv2.FONT_HERSHEY_PLAIN, 2, (0, 255, 0), 3)\n",
    "    \n",
    "    # Update the previous frame time to this frame time.\n",
    "    # As this frame will become previous frame in next iteration.\n",
    "    time1 = time2\n",
    "    \n",
    "    #----------------------------------------------------------------------------------------------------------------------\n",
    "    \n",
    "    # Display the frame.            \n",
    "    cv2.imshow('Subway Surfers with Pose Detection', frame)\n",
    "    \n",
    "    # Wait for 1ms. If a a key is pressed, retreive the ASCII code of the key.\n",
    "    k = cv2.waitKey(1) & 0xFF    \n",
    "    \n",
    "    # Check if 'ESC' is pressed and break the loop.\n",
    "    if(k == 27):\n",
    "        break\n",
    "\n",
    "# Release the VideoCapture Object and close the windows.                  \n",
    "camera_video.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While building big applications like this one, I always divide the application into smaller components and then, in the end, integrate all those components to make the final application. \n",
    "\n",
    "This makes it really easy to learn and understand how everything comes together to build up the full application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  <center> <font style=\"color:rgb(234,19,148)\">Join My Mediapipe Course</font>   </center>\n",
    "\n",
    "You can now join the waitlist for my brand new upcoming course on Mediapipe, I’m not going to any details now but I’m just going to say this course will be a Blast, to say the least. This will be a completely application-oriented course and it will train you on how to create State of the Art exciting applications.\n",
    "\n",
    "<img src='https://drive.google.com/uc?export=download&id=1CP0jp5rlTkOuj23PzUeGGi3NknXVI3wi'>\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<center>  <a href=\"https://www.getdrip.com/forms/677961673/submissions/new\"> <button>Join Now!</button>\n",
    "</a></center>\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
